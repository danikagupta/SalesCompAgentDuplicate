# src/commission_agent.py

import streamlit as st

from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
from src.create_llm_message import create_llm_message

class CommissionAgent:
    
    def __init__(self, model, index):
        """
        Initialize the CommissionAgent with a ChatOpenAI model and a Pinecone index.
        
        :param model: An instance of the ChatOpenAI model used for generating responses.
        :param index: An instance of the Pinecone index, if needed for retrieval (though not used in the basic commission agent).
        """
        self.model = model
        self.index = index

    def generate_commission_response(self, user_query: str) -> str:
        """
        Generate a response for commission-related queries using the ChatOpenAI model.
        
        :param user_query: The original query from the user.
        :return: A string response generated by the language model.
        """
        commission_prompt = f"""
        You are a Sales Commissions expert. Users will ask you about what their commission
        will be for a particular deal. You will use the following method to calculate commission:
        
        Step 1: Check if the user has provided you the Deal value in dollars, on-target incentive (OTI),
        and annual quota in dollars.

        Step 2: If the user did not provide complete information ask them to provide it.

        Step 3: Calculate Base Commission Rate (BCR). BCR is equal to on-target
        incentive divided by annual quota. 
        
        Step 4: Compute the commission by multiplying BCR by deal value. 

        Step 5: Please provide the user with their expected commission in a properly formatted sentence 
        and explain how it was calculated.
        
        
        """

        abc = create_llm_message(commission_prompt)
        msgs=st.session_state.messages
        print(f"COMMISION_AGENT  msgs is {msgs}")
        llm_response = self.model.invoke(abc)
        
        # Generate a response using the ChatOpenAI model
        #llm_response = self.model.invoke([  # Use `invoke` for structured output if needed
        ##    SystemMessage(content="You are an expert in sales compensation and commissions."),
        #    HumanMessage(content=commission_prompt)
        #])
        
        # Extract and return the full response from the language model's output
        full_response = llm_response.content
        return full_response

    def commission_agent(self, state: dict) -> dict:
        """
        Handle commission-related queries by generating a response using the ChatOpenAI model.
        
        :param state: A dictionary containing the state of the current conversation, including the user's initial message.
        :return: A dictionary with the updated state, including the response and the node category.
        """
        # Generate a response based on the user's initial message
        full_response = self.generate_commission_response(state['initialMessage'])
        
        # Return the updated state with the generated response and the category set to 'commission'
        return {
            "lnode": "commission_agent", 
            "responseToUser": full_response,
            "category": "commission"
        }
